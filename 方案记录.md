# 比赛解决方案

## 算力资源
快速申请一个节点用于交互测试
bsub -I -q 2v100 sh 
不要开交互式程序。会卡住没有反应
### 进阶版：使用jupyter lab获得更好的体验
```bash
ipykernel

# Jupyter Lab对比Jupyter Notebook有什么优点和不足？ - Python与数据挖掘的回答 - 知乎
# https://www.zhihu.com/question/413049489/answer/2849948408
black
isort
jupyterlab
jupyterlab-spellchecker
jupyterlab-code-formatter

# https://zhuanlan.zhihu.com/p/383005827
# conda install xeus-python -c conda-forge

# https://www.cnblogs.com/feffery/p/13364668.html#:~:text=2%20jupyter%20lab%E5%AE%9E%E7%94%A8%E6%8F%92%E4%BB%B6%E6%8E%A8%E8%8D%90%201%202.1%20debugger%202%202.2,jupyterlab-execute-time%207%202.7%20jupyterlab-plotly%208%202.8%20jupyterlab-spreadsheet%20%E6%9B%B4%E5%A4%9A%E9%A1%B9%E7%9B%AE
ipympl

```


## 比赛要求的setting是什么？可以发挥什么？不可以发挥什么？
https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af502c4a0ed647faca624e&award=1,000,000
- 属于哪个视觉任务？
    - 什么是单目深度估计？
        - 单个摄像头的图像序列(一张图) -> 场景中物体的深度
        - ？"场景中物体的深度"具体来说是不是实际上是整张图片每个像素点的深度，还是要先识别出一个物体，然后估计这个物体的深度？
    - 什么是跨场景单目深度估计？
        - 源场景图片+源场景深度+目标场景图片 -> 目标场景深度?
        - 错误的，实际上还是图->深度。
        - 训练集分为了多个场景，测试集的场景和训练集的场景不一样。

- 训练集是什么？测试集是什么？有没有标签？
    - 训练集与验证集：6.9W个样本，每个样本包括了什么呢？
        - 是两个图片两个深度？还是一个图片一个深度？
            - 答：是一个图片一个深度
        - 擂主队伍可以加入1000个样本
    - 测试集：2W个样本。不给我们深度？还是说X也不给，只能在线测试？
        - 擂主队伍可以加入1000个样本
- 评审指标
    - 算法精度与算法成本效率，初赛成本不重要，决赛成本20%重要。
    - 精度：REL+(si-RMSE)， 所以训练的时候直接作为loss
    - 成本：存储量+推理时间。
- 允许3个以内的模型集成。


## 复现 ZoeDepth仓库
仓库的需要关注的研究者和实验室是谁？
- Intel Intelligent Systems Lab Org

usage 建议我们用torch.hub拉取 MiDaS 。
- 使用什么版本的软件可以跑通？
    - conda ai环境可以跑通
    - https://github.com/isl-org/ZoeDepth/issues/26 
    - FYI (For your Infomation), timm==0.6.7

- torch hub 是什么原理？
    - https://zhuanlan.zhihu.com/p/68943386
- [MiDaS](https://github.com/isl-org/MiDaS)是什么？
    - Mixing Datasets for Zero-shot Cross-dataset Transfer 融合了很多Dataset知识。
        - MiDaS was trained on up to 12 datasets (ReDWeb, DIML, Movies, MegaDepth, WSVD, TartanAir, HRWSI, ApolloScape, BlendedMVS, IRS, KITTI, NYU Depth V2) with multi-objective optimization
    - monocular 单目视觉
    - dpt_beit_large_512 是最强的模型

- ？ZoeDepth 和 MiDaS的关系是什么？

- 三个模型可以从hub加载， 区别是什么？
    - ZoeD_N
    - ZoeD_K
    - ZoeD_NK

- 模型的方法是什么？
    - 


## 卢师兄去年的方案复现
> https://github.com/isl-org/ZoeDepth这个仓库就是那篇论文的，我们就用他的框架就行。
> 然后把backbone的预训练模型改成这个 omnidata 仓库提供的，我记得他们backbone网络结构一样的，都是DPT，加载omnidata提供的参数就行https://github.com/EPFL-VILAB/omnidata

- omnidata是什么？

- DPT是什么？
https://blog.csdn.net/jiaoyangwm/article/details/121203925
是使用ViT来做深度估计、图像分割的方法。

